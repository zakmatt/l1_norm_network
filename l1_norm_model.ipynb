{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7687829-6172-442e-b618-09926c1f610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7dbbf51-69a6-4272-922e-bf667136f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an mlflow.db file\n",
    "! touch mlflow.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369df3cc-adf4-4712-976e-bb1a6c3ffefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = 'sqlite:///mlflow.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2a1a1-694d-45d9-bdd3-e37eaf6f772a",
   "metadata": {},
   "source": [
    "## Thoughts and assumptions\n",
    "In this task I was to create a network which would compute the $L1$ norm of the a variable-length input sequence of real-valued numbers without using the $L1$ norm explicitly. I could use Dense Layers, Relu activations, the negation operation, and the sum and multiplication operations. No manually initialize a specific weight or set of weights was allowed.\n",
    "\n",
    "I needed to split this problem into multiple 'sub-problems'\n",
    "1. Handle the variable-lenth input sequence.\n",
    "2. Handle values of different signs.\n",
    "3. Build a trainable model which would predict/compute the $L1$ norm of the input.  \n",
    "\n",
    "=====================================================================================================================================\n",
    "\n",
    "1. The first point was relatively easy and hinted in the task description - use an RNN architecture.\n",
    "2. Point number two needed a bit more thinking and not because it was hard to implement it, but in order to better understand why such operations is required.  \n",
    "output of the RNN cell can described using the following equation:  \n",
    "<center>$h' = tanh(W_{ih}*x + b_{ih} + W_{hh}*h + b_{hh})$</center>  \n",
    "If the input $x$ changes its sign, then it will adjust the value that will be passed further before having it processed through the activation function.  I could use the negation so I will change the signs to positive wherever $x < 0$. The main goal is to ensure that all inputs are treated as non-negative before summing, effectively simulating the absolute value operation.  \n",
    "The other thing which requires attention is the activation function itself. $tanh$ value range is $(-1, 1)$. It scales our output and this behaviour is not what we need. We will change it then to relu which returns a linear output for a non-negative input $ReLU(x) = (x)^{+}$. This will preserve positive values as they are. Our equation then changes into\n",
    "<center>$h' = ReLU(W_{ih}*x + b_{ih} + W_{hh}*h + b_{hh})$</center> \n",
    "3. Here I needed to establish what I want to achieve. I have an RNN cell which outputs the sum of the scaled previous output added to the current number which is also scaled. I also add bias. That lead me to the conclusion that the feasible solution to that would get an RNN cel which weights would be identity matrices (or just 1 like in our case; $W_{ih} = 1$ and $W_{hh} = 1$) and bias is 0 (both $b_{ih} = 0$ and $b_{hh} = 0$). I cannot set it manually but at least I know what I'm trying to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e529857c-4393-4f35-9c45-c7c6bbb48841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13ee5ff70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b65b7b-d311-4bff-b79e-5eebfa64d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: tensor([ 1, -1,  2, -2,  3, -3])\n",
      "After: tensor([-1,  1, -2,  2, -3,  3])\n",
      "Convert only negative values\n",
      "tensor([1, 1, 2, 2, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2., 2., 3., 3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor([1, -1, 2, -2, 3, -3])\n",
    "print(f\"Before: {k}\\nAfter: {torch.neg(k)}\")\n",
    "print(\"Convert only negative values\")\n",
    "print(f\"{torch.where(k < 0, torch.neg(k), k)}\")\n",
    "torch.sqrt(k**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5925324-a24a-45b1-bfb8-a5ed57047db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNCell(nn.Module):\n",
    "    \"\"\"Custom RNN cell \n",
    "    \n",
    "    Custom RNN cell which for a given input returns it's positive value summed\n",
    "    to the information carried along.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int = 1, hidden_size: int = 1):\n",
    "        \"\"\"Initialization method\n",
    "\n",
    "        :param input_size: input size; number od values in a single input\n",
    "        :type input_size: int\n",
    "        :param hidden_size: number of features in the hidden layer of our RNN cell\n",
    "        :type hidden_size: int\n",
    "        \"\"\"\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "\n",
    "        self._rnn_cell = nn.RNNCell(\n",
    "            input_size, \n",
    "            hidden_size,\n",
    "            bias=False, # we do not need bias since it's information is irrelevant.\n",
    "            nonlinearity=\"relu\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        :param x: input tensor\n",
    "        :type x: torch.Tensor\n",
    "        :param hidden: output from the previous iteration\n",
    "        :type hidden: torch.Tensor\n",
    "        :return: processed tensor\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # transformation to input x\n",
    "        x = torch.where(x < 0, torch.neg(x), x)\n",
    "        # another option would be to square it and then calculate the root square\n",
    " \n",
    "        # pass through the cell\n",
    "        hidden = self._rnn_cell(x, hidden)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f37f04-2cee-4276-835f-c1254241fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"RNN computing the L1 norm of the input sequence\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int = 1, hidden_size: int = 1):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        :param input_size: input size - in our case it will be one\n",
    "        but other options are also covered\n",
    "        :type: input_size: int\n",
    "        :param hidden_size: \n",
    "        \"\"\"\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = CustomRNNCell(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        :param x: input tensor\n",
    "        :type x: torch.Tensor\n",
    "        :return: computed norm\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "        # Iterate over time steps\n",
    "        for t in range(x.size(1)):\n",
    "            current_input = x[:, t, :]\n",
    "            hidden = self.rnn_cell(current_input, hidden)\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c943b79c-68df-4dc5-af06-d548f20631ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(batch_size: int, max_length: int) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"Generate random sequences and their L1 norms\n",
    "\n",
    "    :param batch_size: batch size\n",
    "    :type batch_size: int\n",
    "    :param max_length: max vector length\n",
    "    :type max_length: int\n",
    "    :return: generated random sequence\n",
    "    :rtype: (torch.Tensor, torch.Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for _ in range(batch_size):\n",
    "        length = torch.randint(1, max_length + 1, (1,)).item() # get random length\n",
    "        seq = torch.randn(length, 1)  # Random sequence of 'length'\n",
    "        l1_norm = torch.sum(torch.abs(seq))  # Compute the L1 norm\n",
    "        sequences.append(seq)\n",
    "        targets.append(torch.tensor([l1_norm], dtype=torch.float32))\n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a74f4e-e7b5-4fe5-a20a-262caf6ef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RNN model\n",
    "input_size = 1  # Each element is a scalar\n",
    "hidden_size = 1  # Output is a scalar\n",
    "model = RNN(input_size=input_size, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa9e4d78-790b-4dab-9475-17cee9d72ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whh = Parameter containing:\n",
      "tensor([[0.8300]], requires_grad=True)\n",
      "Wih = Parameter containing:\n",
      "tensor([[0.7645]], requires_grad=True)\n",
      "bhh = None\n",
      "bih = None\n",
      "sum of coefficients: 1.5945464372634888\n"
     ]
    }
   ],
   "source": [
    "print(f\"Whh = {model.rnn_cell._rnn_cell.weight_hh}\")\n",
    "print(f\"Wih = {model.rnn_cell._rnn_cell.weight_ih}\")\n",
    "print(f\"bhh = {model.rnn_cell._rnn_cell.bias_hh}\")\n",
    "print(f\"bih = {model.rnn_cell._rnn_cell.bias_ih}\")\n",
    "\n",
    "print(f\"sum of coefficients: {sum(param.item() for param in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0202b39-0d52-4d83-8bf3-5055ec2b5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train till w's are 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "839e42cb-b8f7-4c6a-aed4-ee159078fee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/22 00:20:05 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2024/08/22 00:20:05 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
      "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
      "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2024/08/22 00:20:05 INFO mlflow.tracking.fluent: Experiment with name 'RNN L1 Norm Calculation' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 13.961002349853516\n",
      "Epoch 100, Loss: 5.079987049102783\n",
      "Epoch 200, Loss: 0.028426308184862137\n",
      "Epoch 300, Loss: 0.0037581315264105797\n",
      "Epoch 400, Loss: 0.003718046937137842\n",
      "Epoch 500, Loss: 0.005024059675633907\n",
      "Epoch 600, Loss: 0.0016038696048781276\n",
      "Epoch 700, Loss: 0.0031264815479516983\n",
      "Epoch 800, Loss: 0.0065759047865867615\n",
      "Epoch 900, Loss: 0.0038020440842956305\n",
      "Epoch 1000, Loss: 0.0029737164732068777\n",
      "Epoch 1100, Loss: 0.003434132318943739\n",
      "Epoch 1200, Loss: 0.0027860780246555805\n",
      "Epoch 1300, Loss: 0.0015386532759293914\n",
      "Epoch 1400, Loss: 0.0013363079633563757\n",
      "Epoch 1500, Loss: 0.0021456400863826275\n",
      "Epoch 1600, Loss: 0.001322242897003889\n",
      "Epoch 1700, Loss: 0.0013623482082039118\n",
      "Epoch 1800, Loss: 0.001252010464668274\n",
      "Epoch 1900, Loss: 0.0009346791193820536\n",
      "Epoch 2000, Loss: 0.0009686516132205725\n",
      "Epoch 2100, Loss: 0.0007364969933405519\n",
      "Epoch 2200, Loss: 0.0004843974602408707\n",
      "Epoch 2300, Loss: 0.00021630508126690984\n",
      "Epoch 2400, Loss: 0.00020896112255286425\n",
      "Epoch 2500, Loss: 0.0003243033424951136\n",
      "Epoch 2600, Loss: 0.00016487797256559134\n",
      "Epoch 2700, Loss: 0.0002692114212550223\n",
      "Epoch 2800, Loss: 0.00012531883839983493\n",
      "Epoch 2900, Loss: 4.754674228024669e-05\n",
      "Epoch 3000, Loss: 5.401455928222276e-05\n",
      "Epoch 3100, Loss: 3.8384274375857785e-05\n",
      "Epoch 3200, Loss: 5.594881804427132e-05\n",
      "Epoch 3300, Loss: 1.8361144611844793e-05\n",
      "Epoch 3400, Loss: 1.546338535263203e-05\n",
      "Epoch 3500, Loss: 1.5070935660332907e-05\n",
      "Epoch 3600, Loss: 1.0034049410023727e-05\n",
      "Epoch 3700, Loss: 7.889958396845032e-06\n",
      "Epoch 3800, Loss: 6.5774315771705005e-06\n",
      "Epoch 3900, Loss: 1.725713218547753e-06\n",
      "Epoch 4000, Loss: 2.130570237568463e-06\n",
      "Epoch 4100, Loss: 9.506687774774036e-07\n",
      "Epoch 4200, Loss: 5.626180268336611e-07\n",
      "Epoch 4300, Loss: 2.9078012175887125e-07\n",
      "Epoch 4400, Loss: 7.10847771756562e-08\n",
      "Epoch 4500, Loss: 8.49671835112531e-08\n",
      "Epoch 4600, Loss: 5.4955787476274054e-08\n",
      "Epoch 4700, Loss: 1.6576384709310332e-08\n",
      "Epoch 4800, Loss: 1.2872170884747902e-08\n",
      "Epoch 4900, Loss: 3.6751002241430797e-09\n",
      "Epoch 5000, Loss: 1.269830351979806e-09\n",
      "Epoch 5100, Loss: 5.416170845151669e-10\n",
      "Epoch 5200, Loss: 3.251838798234985e-10\n",
      "Epoch 5300, Loss: 1.3125367459565496e-10\n",
      "Epoch 5400, Loss: 5.024515525864359e-11\n",
      "Epoch 5500, Loss: 3.515554514166297e-11\n",
      "Epoch 5600, Loss: 3.759005606784882e-11\n",
      "Epoch 5700, Loss: 2.8484797853778332e-11\n",
      "Epoch 5800, Loss: 2.4062973835725643e-11\n",
      "Epoch 5900, Loss: 1.4965334527161644e-11\n",
      "Epoch 6000, Loss: 4.276233880884384e-12\n",
      "Epoch 6100, Loss: 6.66444677221989e-12\n",
      "Epoch 6200, Loss: 5.3010928979801974e-12\n",
      "Epoch 6300, Loss: 3.799738301779598e-12\n",
      "Epoch 6400, Loss: 5.580313988673424e-12\n",
      "Epoch 6500, Loss: 5.131318980833299e-12\n",
      "Epoch 6600, Loss: 5.2345974776990545e-12\n",
      "Epoch 6700, Loss: 5.121514323747078e-12\n",
      "Epoch 6800, Loss: 5.073524933507656e-12\n",
      "Epoch 6900, Loss: 3.1353808438439046e-12\n",
      "Epoch 7000, Loss: 4.721223412218478e-12\n",
      "Epoch 7100, Loss: 7.808260982233861e-12\n",
      "Epoch 7200, Loss: 8.94368086534758e-12\n",
      "Epoch 7300, Loss: 6.162625965089319e-12\n",
      "Epoch 7400, Loss: 5.769273947464626e-12\n",
      "Epoch 7500, Loss: 6.327178364573527e-13\n",
      "Epoch 7600, Loss: 6.479955461102804e-13\n",
      "Epoch 7700, Loss: 9.08162434143378e-13\n",
      "Epoch 7800, Loss: 1.1729246046643382e-12\n",
      "Epoch 7900, Loss: 1.050277920189302e-12\n",
      "Epoch 8000, Loss: 4.658495811327157e-13\n",
      "Epoch 8100, Loss: 5.308045669671912e-13\n",
      "Epoch 8200, Loss: 4.628120803262803e-13\n",
      "Epoch 8300, Loss: 1.036060126580196e-12\n",
      "Epoch 8400, Loss: 6.050784873146142e-13\n",
      "Epoch 8500, Loss: 7.772671395400721e-13\n",
      "Epoch 8600, Loss: 9.740060320784227e-13\n",
      "Epoch 8700, Loss: 8.103587245678057e-13\n",
      "Epoch 8800, Loss: 4.649614027130156e-13\n",
      "Epoch 8900, Loss: 6.530609386601327e-13\n",
      "Epoch 9000, Loss: 7.501360643757948e-13\n",
      "Epoch 9100, Loss: 7.261862985941114e-12\n",
      "Epoch 9200, Loss: 1.1199648847215471e-11\n",
      "Finished training after 9206 epochs\n",
      "Predicted sum: 7967.36572265625, Actual sum: 7967.38134765625\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "def train_model(model, batch_size, max_length, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"max_length\", max_length)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        epoch = 0\n",
    "        do_train = True\n",
    "        while do_train:\n",
    "            # Generate a batch of random sequences and their L1 norms\n",
    "            sequences, targets = generate_data(\n",
    "                batch_size=batch_size,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            \n",
    "            # Pad sequences to have a consistent batch size\n",
    "            padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "            targets = torch.cat(targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(padded_sequences).squeeze(1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            mlflow.log_metric(\"loss\", loss.item(), step=epoch)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "            # We know what we want to achieve and so we're stopping the training once our weights are one's\n",
    "            if model.rnn_cell._rnn_cell.weight_hh == 1 and model.rnn_cell._rnn_cell.weight_ih == 1:\n",
    "                print(f\"Finished training after {epoch} epochs\")\n",
    "                do_train = False\n",
    "    \n",
    "            if epoch > 5e4:\n",
    "                do_train = False\n",
    "                print(\"Could not converge\")\n",
    "            epoch += 1\n",
    "\n",
    "        mlflow.log_param(\"epochs\", epoch)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "max_length = 10  # Maximum length of any sequence\n",
    "\n",
    "# Track the experiment using MLflow\n",
    "mlflow.set_experiment(\"RNN L1 Norm Calculation\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, batch_size, max_length)\n",
    "\n",
    "# Test the model on a new sequence\n",
    "seq = torch.randn(1, 10000, 1)  # Random sequence of 'length'\n",
    "l1_norm = torch.sum(torch.abs(seq))  # Compute the L1 norm\n",
    "\n",
    "# Get the model's prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_sum = model(seq).item()\n",
    "\n",
    "print(f\"Predicted sum: {predicted_sum}, Actual sum: {l1_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b3ae04-9de1-47c2-8de2-a070f6acd6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whh = Parameter containing:\n",
      "tensor([[1.]], requires_grad=True)\n",
      "Wih = Parameter containing:\n",
      "tensor([[1.]], requires_grad=True)\n",
      "bhh = None\n",
      "bih = None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Whh = {model.rnn_cell._rnn_cell.weight_hh}\")\n",
    "print(f\"Wih = {model.rnn_cell._rnn_cell.weight_ih}\")\n",
    "print(f\"bhh = {model.rnn_cell._rnn_cell.bias_hh}\")\n",
    "print(f\"bih = {model.rnn_cell._rnn_cell.bias_ih}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c526a-6ceb-45fc-8390-6bbcffc72a8c",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "I ran multiple experiments just to check how everything's \n",
    "\n",
    "### No bias, just weights\n",
    "1. both weights are positive, then model converges\n",
    "2. hh positive, ih negative - not converging\n",
    "3. hh negative, ih positive - not converging\n",
    "4. hh negative but very small, ih positive - converging\n",
    "5. both weights negative - not converging\n",
    "\n",
    "\n",
    "### Adding bias\n",
    "1. wh negative, wih positive, bhh positive, bih positive - converges but after a lot of iterations\n",
    "2. wh negative, wih positive, bhh positive, bih negative - sum of coeffs is negative, converges\n",
    "3. wh negative, wih positive, bhh negative, bih negative - sum of coeffs is negative, converges\n",
    "\n",
    "4. wh positive, wih positive (very small), bhh positive, bih negative - converges\n",
    "5. wh positive, wih positive (very small), bhh negative, bih positive - sum of coeffs is positive, converges very fast\n",
    "6. wh positive, wih positive (very small), bhh negative, bih negative - sum of coeffs is positive, converges\n",
    "\n",
    "7. wh positive, wih negative, bhh positive, bih negative - converges\n",
    "8. wh positive, wih negative, bhh negative, bih positive - not converging (sum of coefficients is negative)\n",
    "9. wh positive, wih negative, bhh positive, bih positive - sum of coeffs is positive, converges\n",
    "\n",
    "10. wh negative, wih negative (very small), bhh negative, bih negative - not converging\n",
    "11. wh negative, wih negative (very small), bhh positive, bih negative - converges\n",
    "12. wh negative, wih negative, bhh positive, bih positive - sum of coefficients is negative, converges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963caa8b-aeef-4cab-aa8b-eea5ba19ff2e",
   "metadata": {},
   "source": [
    "# Conclusion and final thoughts\n",
    "The weights should ideally converge to 1, or the identity matrix, since we are essentially adding each input (or its negation) to the accumulated sum.\n",
    "ReLU outputs zero for any input that is negative or zero. If our weights are initialized with negative values, they might lead to negative outputs when applied to the inputs. If these negative outputs are passed through ReLU, they are clamped to zero. In that case we face information loss ($ReLU$ effectively discards negative signals) and gradient issues (if $ReLU$ outputs 0 in the forward pass, its gradient in the backward pass will also be 0. The way to address it would be to make sure that the initialized weights are positive or use an activation function which allows a small, non-zero output for negative inputs. That ensures proper convergence and prevents issues with zero gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2087f-8982-4614-a86c-19ab1fb7a3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l1_norm_network",
   "language": "python",
   "name": "l1_norm_network"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
